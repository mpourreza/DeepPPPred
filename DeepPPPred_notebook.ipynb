{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepPPPred_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qiyr2zLNVVdP"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymUcQ-nGQfQk"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import gensim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.data import load\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import networkx as nx\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "data_path = 'data'\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "validation_df = pd.read_csv(os.path.join(data_path, 'validation.csv'))\n",
        "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
        "\n",
        "pretrained_w2v = gensim.models.Word2Vec.load(os.path.join(data_path, 'word2vec_100_10_5.model'))\n",
        "\n",
        "MAX_LEN = 80\n",
        "SHORT_MAX_LEN = 25\n",
        "MAX_WORDS = 30000\n",
        "OOV_TOKEN = 'OOV'\n",
        "TRUNCATE_MODE = 'post'\n",
        "PADDING_MODE = 'post'\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "class DynamicDataset(Dataset):\n",
        "    def __init__(self, sequences, features, short_sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.features = features\n",
        "        self.short_sequences = short_sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sequences[i], self.features[i], self.short_sequences[i], self.labels[i]) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "with open(os.path.join(data_path, 'sequences_labels.pkl'), 'rb') as handle:\n",
        "    [train_sequences, train_features, train_sp_sequences, train_labels, val_sequences, val_features, val_sp_sequences, val_labels, \n",
        "    test_sequences, test_features, test_sp_sequences, test_labels, propheno_sequences, propheno_features, propheno_labels] = pickle.load(handle)\n",
        "\n",
        "with open(os.path.join(data_path, 'tokenizer.pkl'), 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "\n",
        "weights_matrix = np.zeros((vocab_size+1, EMBEDDING_SIZE))\n",
        "for i, word in enumerate(tokenizer.word_index, start=1):\n",
        "    try: \n",
        "        weights_matrix[i] = pretrained_w2v.wv[word]\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_SIZE, ))\n",
        "\n",
        "train = DynamicDataset(train_sequences, train_features, train_sp_sequences, train_labels)\n",
        "validation = DynamicDataset(val_sequences, val_features, val_sp_sequences, val_labels)\n",
        "test = DynamicDataset(test_sequences, test_features, test_sp_sequences, test_labels)\n",
        "\n",
        "\n",
        "seed = 0\n",
        "\n",
        "class MultiCnn(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        torch.manual_seed(seed)\n",
        "        super(MultiCnn, self).__init__()\n",
        "        ### Original Sentence\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.word_embeddings.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.conv1 = nn.Conv1d(embedding_size, 64, 3)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        self.max_pool1 = nn.MaxPool1d(2)\n",
        "        self.flat1 = nn.Flatten()\n",
        "\n",
        "        self.conv2 = nn.Conv1d(embedding_size, 64, 5)\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "        self.max_pool2 = nn.MaxPool1d(2)\n",
        "        self.flat2 = nn.Flatten()\n",
        "        \n",
        "        ### Shortest Path\n",
        "        self.s_word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.s_word_embeddings.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.s_conv1 = nn.Conv1d(embedding_size, 64, 3)\n",
        "        self.s_drop1 = nn.Dropout(0.3)\n",
        "        self.s_max_pool1 = nn.MaxPool1d(2)\n",
        "        self.s_flat1 = nn.Flatten()\n",
        "\n",
        "        self.s_conv2 = nn.Conv1d(embedding_size, 64, 5)\n",
        "        self.s_drop2 = nn.Dropout(0.3)\n",
        "        self.s_max_pool2 = nn.MaxPool1d(2)\n",
        "        self.s_flat2 = nn.Flatten()\n",
        "        \n",
        "        ### Concatenate\n",
        "        self.fc1 = nn.Linear(64*98, 100)\n",
        "        self.drop4 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(100, 64)\n",
        "        self.drop5 = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, sentence, features, shortest):\n",
        "        embedding = self.word_embeddings(sentence).permute(0, 2, 1)\n",
        "        short_embedding = self.s_word_embeddings(shortest).permute(0, 2, 1)\n",
        "        \n",
        "        conv1 = F.relu(self.conv1(embedding))\n",
        "        drop1 = self.drop1(conv1)\n",
        "        max_pool1 = self.max_pool1(drop1)\n",
        "        flat1 = self.flat1(max_pool1)\n",
        "        \n",
        "        conv2 = F.relu(self.conv2(embedding))\n",
        "        drop2 = self.drop2(conv2)\n",
        "        max_pool2 = self.max_pool2(drop2)\n",
        "        flat2 = self.flat2(max_pool2)\n",
        "    \n",
        "        short_conv1 = F.relu(self.s_conv1(short_embedding))\n",
        "        short_drop1 = self.s_drop1(short_conv1)\n",
        "        short_max_pool1 = self.s_max_pool1(short_drop1)\n",
        "        short_flat1 = self.s_flat1(short_max_pool1)\n",
        "        \n",
        "        short_conv2 = F.relu(self.s_conv2(short_embedding))\n",
        "        short_drop2 = self.s_drop2(short_conv2)\n",
        "        short_max_pool2 = self.s_max_pool2(short_drop2)\n",
        "        short_flat2 = self.s_flat2(short_max_pool2)\n",
        "        \n",
        "        cat = torch.cat((flat1, flat2, short_flat1, short_flat2), dim=1)\n",
        "        \n",
        "        fc1 = F.relu(self.fc1(cat.view(len(sentence), -1)))\n",
        "        drop4 = self.drop4(fc1)\n",
        "        fc2 = F.relu(self.fc2(drop4))\n",
        "        drop5 = self.drop5(fc2)\n",
        "        fc3 = torch.sigmoid(self.fc3(drop5))\n",
        "        \n",
        "        return fc3\n",
        "\n",
        "class BiLSTMShort(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        torch.manual_seed(seed)\n",
        "        super(BiLSTMShort, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.word_embeddings.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.bi_lstm1 = nn.LSTM(embedding_size, 32, bidirectional=True)\n",
        "        self.bi_lstm2 = nn.LSTM(embedding_size, 32, bidirectional=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(64*105, 100)\n",
        "        self.drop1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(100, 64)\n",
        "        self.drop2 = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, sentence, features, shortest):\n",
        "        embedding = self.word_embeddings(sentence)\n",
        "        short_embedding = self.word_embeddings(shortest)\n",
        "        lstm_out1, hidden1 = self.bi_lstm1(embedding)\n",
        "        short_lstm_out1, short_hidden1 = self.bi_lstm2(short_embedding)\n",
        "        cat = torch.cat((lstm_out1.permute(0, 2, 1), short_lstm_out1.permute(0, 2, 1)), dim=2)\n",
        "        \n",
        "        fc1 = F.relu(self.fc1(cat.view(len(sentence), -1)))\n",
        "        drop1 = self.drop1(fc1)\n",
        "        fc2 = F.relu(self.fc2(drop1))\n",
        "        drop2 = self.drop2(fc2)\n",
        "        fc3 = torch.sigmoid(self.fc3(drop2))\n",
        "        return fc3\n",
        "\n",
        "\n",
        "def print_performance(preds, true_labels):\n",
        "    print('Precision: {0:4.3f}, Recall: {1:4.3f}, F1: {2:4.3f}, AUROC: {3:4.3f}'.format(precision_score(true_labels, preds), recall_score(true_labels, preds), f1_score(true_labels, preds), roc_auc_score(true_labels, preds)))\n",
        "    print('tn={0:d}, fp={1:d}, fn={2:d}, tp={3:d}'.format(*confusion_matrix(true_labels, preds).ravel()))\n",
        "    print('{0:4.3f} {1:4.3f} {2:4.3f} {3:4.3f}'.format(precision_score(true_labels, preds), recall_score(true_labels, preds), f1_score(true_labels, preds), roc_auc_score(true_labels, preds)))\n",
        "    \n",
        "def train_model(model, dataset, epochs=20, echo=False):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "    # model.train()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        progress = tqdm_notebook(loader, leave=False) # tqdm_notebook\n",
        "        for inputs, features, short, target in progress:\n",
        "            model.zero_grad()\n",
        "            output = model(inputs.to(device), features.to(device), short.to(device))\n",
        "            loss = criterion(output, target.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if echo:\n",
        "            print(epoch, loss)\n",
        "    return model\n",
        "\n",
        "def concatenate_sequences(sequences, features, shorts, labels, added_sequences, added_features, added_shorts, added_labels):\n",
        "    sequences = torch.cat((sequences, added_sequences))\n",
        "    features = np.concatenate((features, added_features))\n",
        "    shorts = np.concatenate((shorts, added_shorts))\n",
        "    labels = torch.cat((labels, added_labels))\n",
        "    return sequences, features, shorts, labels\n",
        "\n",
        "def eval_model(model, dataset, indices=None, return_binary=False, threshold=None):\n",
        "    if indices is not None:\n",
        "        dataset = DynamicDataset(dataset[indices][0], dataset[indices][1], dataset[indices][2], dataset[indices][3])\n",
        "    \n",
        "    loader = DataLoader(dataset, batch_size=32)\n",
        "    predictions , true_labels = [], []\n",
        "    model.eval()\n",
        "    cnt = 0\n",
        "    for batch in loader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, features, shorts, labels = batch\n",
        "        with torch.no_grad():\n",
        "            logits = model(inputs.to(device), features.to(device), shorts.to(device))\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        cnt += 1\n",
        "        if threshold and cnt == threshold:\n",
        "            break\n",
        "    \n",
        "    predictions = [item for sublist in predictions for item in sublist]\n",
        "    if return_binary:\n",
        "        predictions = np.array([1 if pred[0] > 0.5 else 0 for pred in predictions])\n",
        "    labels = [item[0] for sublist in true_labels for item in sublist]\n",
        "    \n",
        "    return predictions, labels\n",
        "\n",
        "def print_stats(dataset):\n",
        "    print('Length of input dataset: {0:d}'.format(len(dataset)))\n",
        "    print('Positive instances: {0:d} ({1:4.2f}), Negative instances: {2:d} ({3:4.2f})'.format(sum(dataset.labels == 1)[0], int(sum(dataset.labels == 1)[0]) / len(dataset), sum(dataset.labels == 0)[0], int(sum(dataset.labels == 0)[0]) / len(dataset)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_ml0_pPVe6L"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdzeSskZRA6B"
      },
      "source": [
        "seed = 0\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train = DynamicDataset(train_sequences, train_features, train_sp_sequences, train_labels)\n",
        "\n",
        "def run_model(network):\n",
        "    model = network(vocab_size+1, EMBEDDING_SIZE)\n",
        "    model.cuda()\n",
        "    EPOCHS = 20\n",
        "    train_model(model, train, epochs=EPOCHS, echo=False)\n",
        "    return model\n",
        "\n",
        "time1 = time.time()\n",
        "rnn_model = run_model(BiLSTMShort)\n",
        "time2 = time.time()\n",
        "print(time2 - time1)\n",
        "time1 = time.time()\n",
        "cnn_model = run_model(MultiCnn)\n",
        "time2 = time.time()\n",
        "print(time2 - time1)\n",
        "\n",
        "rnn_predictions, true_labels = eval_model(rnn_model, test, return_binary=True)\n",
        "print_performance(rnn_predictions, true_labels)\n",
        "\n",
        "cnn_predictions, true_labels = eval_model(cnn_model, test, return_binary=True)\n",
        "print_performance(cnn_predictions, true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo2uEsFX2EfL"
      },
      "source": [
        "with open(os.path.join(data_path, 'pppred_bert_probabilities_validation_test.pkl'), 'rb') as handle:\n",
        "    [flat_predictions_val, flat_predictions_test] = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8w92SjXqPrL"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "rnn_val_predictions, true_labels = eval_model(rnn_model, validation, return_binary=False)\n",
        "rnn_val_predictions = np.array(rnn_val_predictions)\n",
        "cnn_val_predictions, true_labels = eval_model(cnn_model, validation, return_binary=False)\n",
        "cnn_val_predictions = np.array(cnn_val_predictions)\n",
        "probabilities = clf.predict_proba(vec.transform(validation_df['Sentence']))\n",
        "lr = LogisticRegression()\n",
        "lr.fit(np.concatenate((rnn_val_predictions, cnn_val_predictions, probabilities[:,1].reshape(-1,1), flat_predictions_val.reshape(-1,1)), axis=1), val_labels)\n",
        "\n",
        "rnn_test_predictions, true_labels = eval_model(rnn_model, test, return_binary=False)\n",
        "rnn_test_predictions = np.array(rnn_test_predictions)\n",
        "cnn_test_predictions, true_labels = eval_model(cnn_model, test, return_binary=False)\n",
        "cnn_test_predictions = np.array(cnn_test_predictions)\n",
        "probabilities_test = clf.predict_proba(vec.transform(test_df['Sentence']))\n",
        "lr_preds = lr.predict(np.concatenate((rnn_test_predictions, cnn_test_predictions, probabilities_test[:,1].reshape(-1,1), flat_predictions_test.reshape(-1,1)), axis=1))\n",
        "lr_preds = np.array(lr_preds)\n",
        "true_labels = np.array(true_labels)\n",
        "print_performance(lr_preds, true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqJkmY2AKjV4"
      },
      "source": [
        "abs_idx = np.where(test_df['type'] == 'abs')\n",
        "ft_idx = np.where(test_df['type'] == 'ft')\n",
        "\n",
        "print_performance(preds, true_labels)\n",
        "print_performance(preds[abs_idx], true_labels[abs_idx])\n",
        "print_performance(preds[ft_idx], true_labels[ft_idx])\n",
        "print('*' * 50)\n",
        "print_performance((flat_predictions_test > 0), test_labels)\n",
        "print_performance((flat_predictions_test > 0)[abs_idx], test_labels[abs_idx])\n",
        "print_performance((flat_predictions_test > 0)[ft_idx], test_labels[ft_idx])\n",
        "print('*' * 50)\n",
        "print_performance(lr_preds, true_labels)\n",
        "print_performance(lr_preds[abs_idx], true_labels[abs_idx])\n",
        "print_performance(lr_preds[ft_idx], true_labels[ft_idx])\n",
        "print('*' * 50)\n",
        "print_performance(rnn_predictions, true_labels)\n",
        "print_performance(rnn_predictions[abs_idx], true_labels[abs_idx])\n",
        "print_performance(rnn_predictions[ft_idx], true_labels[ft_idx])\n",
        "print('*' * 50)\n",
        "print_performance(cnn_predictions, true_labels)\n",
        "print_performance(cnn_predictions[abs_idx], true_labels[abs_idx])\n",
        "print_performance(cnn_predictions[ft_idx], true_labels[ft_idx])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}